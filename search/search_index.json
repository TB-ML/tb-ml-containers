{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"TB-ML-Containers","text":"<p>Welcome to the tb-ml-community page which serves as a centralised repository list of containers that can be used with tb-ml. Click on an individual container to find out more. For information on how to build containers and how to add them to this page, click on the contribute section.</p>"},{"location":"#prediction-containers","title":"Prediction containers","text":"Name Architecture Drugs Input Docker TB-AMR-CNN CNN amikacin, capreomycin, ciprofloxacin, ethambutol, ethionamide, isoniazid, kanamycin, levofloxacin, moxifloxacin, ofloxacin, pyrazinamide, rifampicin, streptomycin One-hot-encoded-sequence/CSV julibeg/tb-ml-neural-net-from-one-hot-encoded-seqs-13-drugs tb-dr-pred-nn CNN amikacin, capreomycin, ciprofloxacin, ethambutol, ethionamide, isoniazid, kanamycin, levofloxacin, moxifloxacin, ofloxacin, pyrazinamide, rifampicin, streptomycin One-hot-encoded-sequence/CSV linfengwang/tb-dr-pred-nn aggreen-MTB-CNN CNN amikacin, capreomycin, ciprofloxacin, ethambutol, ethionamide, isoniazid, kanamycin, levofloxacin, moxifloxacin, ofloxacin, pyrazinamide, rifampicin, streptomycin consensus-sequences/FASTA julibeg/tb-ml-aggreen-mtb-cnn RF-Streptomycin Random Forest streptomycin variants/CSV julibeg/tb-ml-random-forest-from-variants-streptomycin mykrobe Direct Association amikacin, capreomycin, ciprofloxacin, delamanid, ethambutol, ethionamide, isoniazid, kanamycin, levofloxacin, linezolid, moxifloxacin, ofloxacin, pyrazinamide, rifampicin, streptomycin fastq jodyphelan/mykrobe"},{"location":"#preprocessing-containers","title":"Preprocessing containers","text":"Name Input Features Docker raw2consensus fastq consensus-sequences/FASTA julibeg/tb-ml-consensus-seqs-from-raw-reads aln2variants sam/bam/cram variants julibeg/tb-ml-variants-from-aligned-reads raw2one-hot fastq One-hot-encoded-sequence/CSV julibeg/tb-ml-one-hot-encoded-seqs-from-raw-reads aln2one-hot sam/bam One-hot-encoded-sequence/CSV julibeg/tb-ml-one-hot-encoded-seqs-from-aligned-reads"},{"location":"contribute/","title":"Contribute","text":""},{"location":"contribute/#how-to-develop-a-container","title":"How to develop a container","text":"<p>A container is essentially a mini virtual machine that contains all software needed to run predictions on new data. Docker is used to build the containers and they can then be run on any system which has docker installed. There are a few steps required to build a container. We'll go through these by going through a whole example, from training to prediction. </p>"},{"location":"contribute/#step-1-training-the-model","title":"Step 1 - Training the model","text":"<p>As an example we will train a random forest classifier to predict resistance to rifampicin. The input data is a set of 100 resistant and 100 sensitive isolates randomly taken from the SRA. The input data and scripts are available from this repository. </p> <p>Warning</p> <p>This is a very quick and dirty model that takes a lot of shortcuts, it is just meant for demonstration purposes.</p> <p>First lets set up a conda environment for our code:</p> <pre><code>mamba create -n ml -c conda-forge -c bioconda scikit-learn bcftools\nconda activate ml\n</code></pre> <p>This is our training code.</p> <pre><code>from sklearn.ensemble import RandomForestClassifier\nimport pickle\nimport subprocess as sp\nfrom collections import defaultdict\n\ngenos = defaultdict(dict)\n\n# get genotypes for all samples at all positions in the vcf\nfor l in sp.Popen(r\"bcftools query -f '[%POS\\t%SAMPLE\\t%GT\\n]' snps.vcf.gz\",shell=True,stdout=sp.PIPE).stdout:\n    # l is a byte string, so we need to decode it to a string and strip the newline character at the end\n    pos,sample,gt = l.decode().strip().split()\n    # convert the position to an integer\n    pos = int(pos)\n    # if the genotype is missing or reference, set it to 0 (otherwise set it to 1)\n    if gt==\"./.\" or gt==\"0/0\":\n        gt = 0\n    else:\n        gt = 1\n    # add the genotype to the dictionary\n    genos[sample][pos] = gt\n\n# get the phenotype for each sample\npheno = {}\nfor l in open(\"pheno.txt\"):\n    row = l.strip().split()\n    pheno[row[0]] = int(row[1])\n\n# convert the genotype dictionary to a list of lists\nX = [list(genos[s].values()) for s in pheno] \ny = [pheno[s] for s in pheno]\n# use scikit-learn to train a random forest classifier\nclf = RandomForestClassifier(max_depth=2, random_state=0)\nclf.fit(X, y)\n\n# dump the model and the positions to a pickle file\npositions = list(list(genos.values())[0])\npickle.dump({\"model\":clf,\"positions\":positions}, open(\"model.pkl\",\"wb\"))\n</code></pre> <p>Note</p> <p>We have used pickle to dump the model to a file. Importantly, we also include the genomic positions of all the features so it will allow us to re-create the input data in the exact same format. We can then use the <code>.predict()</code> method from the object when we reload it somewhere else.</p> <p>Execute the code to run the training:</p> <pre><code>$ python train.py\n</code></pre>"},{"location":"contribute/#step-2-set-up-your-prediction-script","title":"Step 2 - Set up your prediction script","text":"<p>Now that we have the model saved, we can create a script to load it along with a vcf from a new sample and make a prediction. Here is the code for that.</p> <pre><code>\"\"\" read in genotypes from VCF file and predict outcome \"\"\"\nimport sys\nimport os\nimport argparse\nimport pickle\nimport subprocess as sp\n\n# parse command line arguments\nparser = argparse.ArgumentParser(description='Predict outcome from VCF file')\nparser.add_argument('--vcf', help='VCF file', required=True)\nparser.add_argument('--model', help='model file', required=True)\nargs = parser.parse_args()\n\n# load model\nitems = pickle.load(open(args.model, \"rb\"))\nclf = items['model']\npositions = items['positions']\n\n\n\n# extract genotypes\ngenos = {}\nfor l in sp.Popen(r\"bcftools query -f '%POS[\\t%GT\\n]' \"+ args.vcf,shell=True,stdout=sp.PIPE).stdout:\n    row = l.decode().strip().split()\n    if row[1]!=\"0/0\":\n        genos[int(row[0])] = 1\n    else:\n        genos[int(row[0])] = 0\n\n# fill in missing genotypes with 0 (ref)\nfor p in positions:\n    if p not in genos:\n        genos[p] = 0\n\ngenos = [[genos[p] for p in positions]]\n\n# predict outcome\npred = clf.predict(genos)\n# write output\nsys.stdout.write(\"drug,prediction\\nrifampicin,%s\\n\" % pred[0])\n</code></pre> <p>Note</p> <p>If you are making your container to be complient with <code>tb-ml</code> you need to write any results you want in the final output to stdout.</p> <p>We can then use this script to predict on some new data.</p> <pre><code>$ python predict.py --vcf por5A1.vcf.gz --model model.pkl\ndrug,prediction\nrifampicin,1\n</code></pre> <p>Now we are ready to package our prediction pipeline into a container.</p>"},{"location":"contribute/#step-3-create-container","title":"Step 3 - Create container","text":"<p>To build a container you first need docker installed. Head over to https://www.docker.com/ to download the latest version. Once you have it installed you can start the build process. First we need to define what software we need. It is very important to use the exact same versions of libraries and software in the docker container as you used to train. You can check this with conda.</p> <pre><code>$ conda list | grep bcftools\nbcftools                  1.16                 h83fc8ca_1    bioconda\n$ conda list | grep scikit-learn\nscikit-learn              1.2.1           py311h087fafe_0    conda-forge\n</code></pre> <p>So we can see we need to use <code>bcftools=1.16</code> and <code>scikit-learn=1.2.1</code>. To build our container we need to create a file called <code>Dockerfile</code> and insert our software dependencies. We also need to add our saved model file and the prediction script. </p> <pre><code>FROM mambaorg/micromamba:latest\nLABEL image.name=\"jodyphelan/simple-rif-rf\"\n\n# This needs to be set for the PATH variable to be set correctly\nARG MAMBA_DOCKERFILE_ACTIVATE=1\n\n# Install our software\nRUN micromamba install -y -n base -c bioconda -c conda-forge \\\n    scikit-learn=1.2.1 \\\n    bcftools=1.16 \\\n    tqdm &amp;&amp; \\\n    micromamba clean --all --yes\n\n# create a directory for the internal data used by the container\nUSER root\nRUN mkdir /internal_data /data\n\n# copy the model, data files, and scripts\nCOPY model.pkl /internal_data/model.pkl\nCOPY predict.py /internal_data\n\n# set `/data` as working directory so that the output is written to the\n# mount point when run with `docker run -v $PWD:/data ... -o output.csv`\nWORKDIR /data\n\nENTRYPOINT [\"/usr/local/bin/_entrypoint.sh\", \"python\", \"/internal_data/predict.py\", \"--model\", \"/internal_data/model.pkl\"]\n</code></pre> <p>We can then build the container with </p> <pre><code>$ docker build -t simple-rif-rf . </code></pre> <p>In a nutshell, this installs our required software, copies the data files and scripts to the container and finally defines an \"entrypoint\" script that will be run when the container is executed.</p> <p>We can then use this container in a similar way to how we used the script to predict. Since, we already point the <code>--model</code> flag to the right file in the ENTRYPOINT variable we only need to give the <code>--vcf</code> argument.</p> <pre><code>$ docker run --rm -it -v $PWD:/data  simple-rif-rf  \ndrug,prediction\nrifampicin,1\n</code></pre> <p>Now finally you can publish your container to dockerhub. Head over to https://hub.docker.com/ and set yourself up with an account. Once you have done this you can link your account with your local docker installation.</p> <pre><code>$ docker login\n</code></pre> <p>And finally you can push the container to dockerhub</p> <pre><code>$ docker push jodyphelan/simple-rif-rf:latest\n</code></pre>"},{"location":"contribute/#adding-your-container-to-tb-ml-containers","title":"Adding your container to TB-ML-Containers","text":"<p>If you want to publicise your container you can add it to this website to improve its visibility. To do this you just have to fork this repo. After this, you should add a markdown file to the appropriate location <code>/docs/containers/prediction</code> for prediction containers or <code>/docs/containers/preprocessing</code> for preprocessing containers. The markdown file should contain a few variables that are defined at the top of the page in yaml format. In particular, for prediction containers they need:</p> <ul> <li>title - Name of the model</li> <li>drugs - A list of drugs it predicts for</li> <li>architecture - The type of ML model</li> <li>input - The input type it needs (e.g. VCF)</li> <li>docker - The name of the container on dockerhub</li> </ul> <p>After defining these you can write about your model using standard markdown. For example:</p> <pre><code>---\ntitle: simple-rif-rf\ndrugs: ['rifampicin']\narchitecture: RandomForest\ndocker: jodyphelan/simple-rif-rf\ninput: variants/VCF\n---\n\n# Rifampicin RandomForest\n\nThis container predicts rifampicin resistance from variants in VCF format.\n</code></pre> <p>Now just put in a pull request and we will add it in to the website. You model will automatically appear on the tables on the front page. A dedicated rendering your markdown content will also be generated using the markdown filename as the link.</p>"},{"location":"support/","title":"Support","text":"<p>This work has kindly been supported by the UKRI MRC (Ref. MR/X005895/1).</p> <p></p>"},{"location":"containers/prediction/RF-Streptomycin/","title":"RF-Streptomycin","text":"<p>This container holds a random forest model trained on Mtb variants in order to predict resistance against streptomycin. It can be queried to give the list of variants required for prediction (see usage examples below). The workdir in the container is <code>/data</code> which is also where the output file will be generated (and thus it needs to be mounted as a volume on the host).</p>"},{"location":"containers/prediction/RF-Streptomycin/#usage-examples","title":"Usage examples","text":""},{"location":"containers/prediction/RF-Streptomycin/#getting-the-target-variants-necessary-for-prediction","title":"Getting the target variants necessary for prediction","text":"<p>This will create <code>target-vars.csv</code> with the list of variants required for prediction and their allele frequencies in the training dataset (so that non-calls can be replaced by the allele frequency of the corresponding variant). The columns of the CSV will be <code>POS,REF,ALT,AF</code>.</p> <pre><code>docker run -v $PWD:/data \\\njulibeg/tb-ml-random-forest-from-variants-streptomycin:v0.4.0 \\\n--get-target-vars -o target-vars.csv\n</code></pre>"},{"location":"containers/prediction/RF-Streptomycin/#prediction","title":"Prediction","text":"<p>The container expects a CSV with the genotypes of the target variants and the header line <code>POS,REF,ALT,GT</code>. The file needs to have the same <code>POS</code>, <code>REF</code>, and <code>ALT</code> columns as the file written by <code>--get-target-vars</code>.</p> <pre><code>docker run -v $PWD:/data \\\njulibeg/tb-ml-random-forest-from-variants-streptomycin:v0.4.0 \\\nmy-variants.csv\n</code></pre>"},{"location":"containers/prediction/TB-AMR-CNN/","title":"TB-AMR-CNN","text":"<p>The NN was trained using data from MTB-CNN and uses a similar architecture with the distinction that no multi-sequence alignment is needed (for more details see here). Instead, it requires only the one-hot-encoded sequences of the relevant loci from a particular sample.</p> <p>The start and end coordinates of the loci can be found in <code>data_files/target_loci.csv</code> or queried from the container by passing the <code>--get-target-loci</code> argument (see below). If you have a SAM/BAM/CRAM file with reads aligned against H37Rv, you can use this Docker container to extract the one-hot-encoded sequences.</p>"},{"location":"containers/prediction/TB-AMR-CNN/#example-usage","title":"Example usage","text":"<p>Get coordinates of target loci</p> <pre><code>docker run -v $PWD:/data \\\njulibeg/tb-ml-neural-net-from-one-hot-encoded-seqs-13-drugs:v0.7.0 \\\n--get-target-loci \\\n-o nn_target_loci.csv\n</code></pre> <p>Predict resistance against 13 drugs from one-hot-encoded sequences (passed in a CSV file)</p> <pre><code>docker run -v $PWD:/data \\\njulibeg/tb-ml-neural-net-from-one-hot-encoded-seqs-13-drugs:v0.7.0 \\\ninput_seqs.csv\n</code></pre>"},{"location":"containers/prediction/TB-DR-pred-nn/","title":"Small CNN model for prediction TB drug resistance from one-hot-encoded sequences of target loci","text":"<p>Model adapted from: Green, A.G. et al. (2021) \u201cA convolutional neural network highlights mutations relevant to antimicrobial resistance in Mycobacterium tuberculosis.\u201d; Available at: https://doi.org/10.1101/2021.12.06.471431.</p> <p>Compared to the original, this model is reduced in size (1/3 original size), but retains comparable accuracy (~93%).</p> <p>Input one-hot-encoded consensus sequences of the following loci: acpM-kasA, gid, rpsA, clpC, embCAB, aftB-ubiA, rrs-rrl, ethAR, oxyR-ahpC, tlyA, katG, rpsLrpoBC, fabG1-inhA, eis, gyrBA, panD, pncA.</p>"},{"location":"containers/prediction/TB-DR-pred-nn/#example-usage","title":"Example usage","text":"<p>Get the one-hot-encoded sequences from a BAM file</p> <pre><code>docker run -v $PWD:/data \\\n    linfengwang/tb-ml-one-hot-encoded-seqs-from-raw-reads-with-gap-insertion \\\n    -b file.bam \\\n    -o nn_target_loci.csv\n</code></pre> <p>Predict resistance against 13 drugs from the one-hot-encoded sequences</p> <pre><code>docker run -v $PWD:/data \\\n    linfengwang/tb-dr-pred-nn \\\n    input_seqs.csv\n</code></pre>"},{"location":"containers/prediction/aggreen-MTB-CNN/","title":"aggreen-MTB-CNN","text":"<p>The model was taken from Green &amp; Yoon et al., Nat Comms (2022).</p> <p>The start and end coordinates of the 13 target loci can be found in <code>data_files/target_loci.csv</code> or queried from the container by passing the <code>--get-target-loci</code> argument (see below). Consensus sequences from raw reads can be created with this Docker container</p>"},{"location":"containers/prediction/aggreen-MTB-CNN/#example-usage","title":"Example usage","text":"<p>Get coordinates of target loci</p> <pre><code>docker run -v $PWD:/data \\\njulibeg/tb-ml-aggreen-mtb-cnn:v0.1.0 \\\n--get-target-loci \\\n-o nn_target_loci.csv\n</code></pre> <p>Predict resistance against 13 drugs from target consensus sequences (passed as FASTA file)</p> <pre><code>docker run -v $PWD:/data \\\njulibeg/tb-ml-aggreen-mtb-cnn:v0.1.0 \\\ninput_seqs.fa\n</code></pre>"},{"location":"containers/prediction/mykrobe/","title":"mykrobe","text":"<p>This container wraps around the <code>mykrobe</code> prediction tool that reads in fastq data and performs resistance prediction to a multitude of drugs using the direct association method.</p> <p>For more information visit https://github.com/Mykrobe-tools/mykrobe.</p>"},{"location":"containers/preprocessing/aln2onehot/","title":"aln2one-hot","text":"<p>The container uses <code>sambamba</code> to transform target consensus sequences extracted from aligned reads into one-hot encoding. The start and end coordinates of the sequences are read from a CSV file which is required and must have the header line <code>locus,start,end</code>. The sequences are concatenated without gaps.</p>"},{"location":"containers/preprocessing/aln2onehot/#usage","title":"Usage","text":"<p>A SAM/BAM file with aligned reads against H37Rv and a CSV specifying target loci are required as input. The container uses <code>/data</code> as working directory and will create the output file there.</p> <pre><code>docker run -v $PWD:/data \\\njulibeg/tb-ml-one-hot-encoded-seqs-from-aligned-reads:v0.4.0 \\\n-b aligned_reads.bam \\\n-r target_loci.csv \\\n-o one_hot_seqs.csv\n</code></pre>"},{"location":"containers/preprocessing/aln2variants/","title":"aln2variants","text":"<p>This example container uses Freebayes to call variants from a sorted BAM/CRAM file with reads aligned against H37Rv ASM19595v2. In addition to the aligned reads, it also expects a CSV with target variants for which there must be a genotype in the output file. This is needed for prediction containers relying on a pre-determined set of variants as input data. The header line of the CSV should be <code>POS,REF,ALT,AF</code> with the <code>AF</code> column holding allele frequencies that can be used to replace non-calls. </p> <p>It will write the called variants to a CSV with the columns <code>POS,REF,ALT,GT</code>.</p>"},{"location":"containers/preprocessing/aln2variants/#usage","title":"Usage","text":"<p>The container uses <code>/data</code> as working directory and will create the output file there. It can be called like this:</p> <pre><code>docker run -v $PWD:/data \\\njulibeg/tb-ml-variants-from-aligned-reads:v0.4.0 \\\n-b aligned-reads.bam \\\n-t target-vars.csv \\\n-o called-variants.csv\n</code></pre>"},{"location":"containers/preprocessing/raw2consensus/","title":"raw2consensus","text":"<p>This container generates consensus sequences of M. tuberculosis raw reads in a list of target regions. It requires the reads, a CSV file with coordinates of the target regions, and a name for the output FASTA file. <code>bwa-mem2</code> is used for aligning the reads against H37Rv (asm19595v2) and variants are called with <code>freebayes</code>. The CSV file with the target regions should have the header line 'locus,start,end'.</p>"},{"location":"containers/preprocessing/raw2consensus/#usage","title":"Usage","text":"<p>FASTQ files with forward and reverse M tuberculosis reads and a CSV file specifying the target loci are required as input. The container uses <code>/data</code> as working directory and will create the output file there.</p> <pre><code>docker run -v $PWD:/data \\\njulibeg/tb-ml-consensus-seqs-from-raw-reads:v0.1.1 \\\n-r target_loci.csv \\\n-o consensus_seqs.fasta \\\nmy-sample_1.fastq.gz \\\nmy-sample_2.fastq.gz\n</code></pre>"},{"location":"containers/preprocessing/raw2onehot/","title":"raw2one-hot","text":"<p>The container uses <code>bwa-mem2 mem</code> to align raw reads to the M tuberculosis H37Rv reference genome (ASM19595v2) and afterwards extracts one-hot-encoded consensus sequences of a list of target loci. The start and end coordinates of the target sequences are read from a CSV file which is required and must have the header line <code>locus,start,end</code>. The sequences are concatenated without gaps.</p>"},{"location":"containers/preprocessing/raw2onehot/#usage","title":"Usage","text":"<p>FASTQ files with forward and reverse M tuberculosis reads and a CSV file specifying the target loci are required as input. The container uses <code>/data</code> as working directory and will create the output file there.</p> <pre><code>docker run -v $PWD:/data \\\njulibeg/tb-ml-one-hot-encoded-seqs-from-raw-reads:v0.2.0 \\\n-r target_loci.csv \\\n-o one_hot_seqs.csv \\\nmy-sample_1.fastq.gz \\\nmy-sample_2.fastq.gz\n</code></pre>"}]}